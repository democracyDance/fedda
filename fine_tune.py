import torch
import torch.nn as nn
import torch.optim as optim
from model.feature_extractor import FeatureExtractor
from model.classifier import Classifier
from utils.data_loader import get_loader
from utils.cli_parser import get_args
import config
from tqdm import tqdm
import os
#å¾®è°ƒé˜¶æ®µåªé’ˆå¯¹ç›®æ ‡åŸŸåˆ†ç±»å™¨ï¼Œå†»ç»“ç‰¹å¾æå–å™¨ï¼Œé¿å…è¿‡æ‹Ÿåˆ

# è¯„ä¼°å‡½æ•°
def evaluate(model, classifier, dataloader, device):
    model.eval()
    classifier.eval()
    total_acc = 0
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            feat = model(x)
            logits = classifier(feat)
            acc = (logits.argmax(1) == y).float().mean().item()
            total_acc += acc
    return total_acc / len(dataloader)


def fine_tune():
    args = get_args()
    dataset = args.dataset.lower()
    target_domain = args.target
    device = torch.device(args.device)

    print("\nğŸ¯ Fine-tune å¾®è°ƒé˜¶æ®µå¼€å§‹...")

    # === åŠ è½½è®­ç»ƒå¥½çš„å…¨å±€æ¨¡å‹å‚æ•° ===
    extractor = FeatureExtractor().to(device)
    classifier = Classifier().to(device)
    extractor.load_state_dict(torch.load("out/global_extractor.pth"))
    classifier.load_state_dict(torch.load("out/global_classifier.pth"))

    # === å†»ç»“ç‰¹å¾æå–å™¨å‚æ•°ï¼Œåªè®­ç»ƒåˆ†ç±»å™¨ ===
    for param in extractor.parameters():
        param.requires_grad = False

    # === ä¼˜åŒ–å™¨ä»…æ›´æ–°åˆ†ç±»å™¨ ===
    optimizer = optim.Adam(classifier.parameters(), lr=config.FINETUNE_LR)
    criterion = nn.CrossEntropyLoss()

    # === å‡†å¤‡ç›®æ ‡åŸŸæ•°æ® ===
    target_loader = get_loader(os.path.join(os.getcwd(), "data", dataset), target_domain, batch_size=config.BATCH_SIZE)

    # === Fine-tune å¼€å§‹ ===
    classifier.train()
    for epoch in range(args.epochs):
        total_loss, total_acc = 0, 0
        data_iterator = tqdm(target_loader, desc=f"Fine-tune Epoch {epoch + 1}")
        for x, y in data_iterator:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():
                feat = extractor(x)
            logits = classifier(feat)
            loss = criterion(logits, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_acc += (logits.argmax(1) == y).float().mean().item()
            data_iterator.set_postfix(loss=total_loss / (len(target_loader)),
                                      acc=total_acc / (len(target_loader)))

        print(f"Epoch {epoch + 1}: Loss={total_loss / len(target_loader):.4f}, "
              f"Acc={total_acc / len(target_loader) * 100:.2f}%")

    # === ä¿å­˜å¾®è°ƒåçš„åˆ†ç±»å™¨å‚æ•° ===
    torch.save(classifier.state_dict(), "out/fine_tuned_classifier.pth")
    print("âœ… Fine-tune å®Œæˆï¼Œåˆ†ç±»å™¨å·²ä¿å­˜ â†’ out/fine_tuned_classifier.pth")

    # === ğŸ¯ å¾®è°ƒåéªŒè¯é˜¶æ®µ ===
    print("\nğŸ¯ å¼€å§‹ç›®æ ‡åŸŸéªŒè¯è¯„ä¼°...")
    extractor.eval()
    classifier.eval()
    final_acc = evaluate(extractor, classifier, target_loader, device)
    print(f"âœ… ç›®æ ‡åŸŸéªŒè¯å‡†ç¡®ç‡ï¼š{final_acc * 100:.2f}%")

if __name__ == "__main__":
    fine_tune()


